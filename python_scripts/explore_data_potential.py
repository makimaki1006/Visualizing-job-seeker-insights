#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ‡ãƒ¼ã‚¿ã®æ½œåœ¨çš„ä¾¡å€¤ã‚’æ¢ç´¢ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€ã¾ã æ´»ç”¨ã•ã‚Œã¦ã„ãªã„æ¬¡å…ƒã‚„é–¢ä¿‚æ€§ã‚’ç™ºè¦‹ã™ã‚‹
"""

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from collections import defaultdict
from run_complete_v2_perfect import PerfectJobSeekerAnalyzer


def explore_data_dimensions(analyzer):
    """ãƒ‡ãƒ¼ã‚¿ã®æ¬¡å…ƒã‚’æ¢ç´¢"""

    print("=" * 100)
    print("ã€ãƒ‡ãƒ¼ã‚¿æ¬¡å…ƒã®æ¢ç´¢ã€‘")
    print("=" * 100)

    df = analyzer.processed_data

    # åˆ©ç”¨å¯èƒ½ãªæ¬¡å…ƒã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—
    dimensions = {
        'åŸºæœ¬å±æ€§': ['age', 'age_bucket', 'gender'],
        'åœ°ç†': ['residence_pref', 'residence_muni', 'desired_areas'],
        'è³‡æ ¼ãƒ»ã‚¹ã‚­ãƒ«': ['qualifications', 'qualification_count', 'has_national_license'],
        'ã‚­ãƒ£ãƒªã‚¢': ['career', 'employment_status', 'desired_job'],
        'å¸Œæœ›æ¡ä»¶': ['desired_workstyle', 'desired_start', 'status']
    }

    print("\nã€åˆ©ç”¨å¯èƒ½ãªåˆ†æè»¸ã€‘")
    for category, dims in dimensions.items():
        print(f"\nâ–  {category}")
        for dim in dims:
            if dim in df.columns:
                if dim == 'desired_areas' or dim == 'qualifications':
                    unique_count = "ãƒªã‚¹ãƒˆå‹"
                else:
                    unique_count = df[dim].nunique()
                print(f"  - {dim}: {unique_count}ç¨®é¡")

    # å¤šæ¬¡å…ƒã‚¯ãƒ­ã‚¹é›†è¨ˆã®å¯èƒ½æ€§ã‚’æ¤œè¨¼
    print("\n\n" + "=" * 100)
    print("ã€å¤šæ¬¡å…ƒã‚¯ãƒ­ã‚¹é›†è¨ˆã®å¯èƒ½æ€§ã€‘")
    print("=" * 100)

    # 3æ¬¡å…ƒã‚¯ãƒ­ã‚¹é›†è¨ˆã®ã‚µãƒ³ãƒ—ãƒ«
    print("\nã€3æ¬¡å…ƒ: å¸‚ç”ºæ‘ Ã— å¹´é½¢å±¤ Ã— æ€§åˆ¥ã€‘")

    # å¸Œæœ›å‹¤å‹™åœ°ã‚’å±•é–‹
    desired_munis = []
    for idx, row in df.iterrows():
        for area in row['desired_areas']:
            desired_munis.append({
                'target_muni': area['full'],
                'age_bucket': row['age_bucket'],
                'gender': row['gender'],
                'has_national_license': row['has_national_license'],
                'qualification_count': row['qualification_count'],
                'employment_status': row['employment_status'],
                'residence_pref': row['residence_pref']
            })

    desired_df = pd.DataFrame(desired_munis)

    # 3æ¬¡å…ƒé›†è¨ˆ
    cross3d = desired_df.groupby(['target_muni', 'age_bucket', 'gender']).size().reset_index(name='count')
    cross3d = cross3d.sort_values('count', ascending=False)

    print(f"  - ç·çµ„ã¿åˆã‚ã›æ•°: {len(cross3d):,}é€šã‚Š")
    print(f"  - ä¸Šä½10çµ„ã¿åˆã‚ã›:")
    print(cross3d.head(10).to_string(index=False))

    # 4æ¬¡å…ƒã‚¯ãƒ­ã‚¹é›†è¨ˆ
    print("\nã€4æ¬¡å…ƒ: å¸‚ç”ºæ‘ Ã— å¹´é½¢å±¤ Ã— æ€§åˆ¥ Ã— å›½å®¶è³‡æ ¼æœ‰ç„¡ã€‘")
    cross4d = desired_df.groupby(['target_muni', 'age_bucket', 'gender', 'has_national_license']).size().reset_index(name='count')
    cross4d = cross4d.sort_values('count', ascending=False)

    print(f"  - ç·çµ„ã¿åˆã‚ã›æ•°: {len(cross4d):,}é€šã‚Š")
    print(f"  - ä¸Šä½10çµ„ã¿åˆã‚ã›:")
    print(cross4d.head(10).to_string(index=False))

    # 5æ¬¡å…ƒã‚¯ãƒ­ã‚¹é›†è¨ˆ
    print("\nã€5æ¬¡å…ƒ: å¸‚ç”ºæ‘ Ã— å¹´é½¢å±¤ Ã— æ€§åˆ¥ Ã— å›½å®¶è³‡æ ¼ Ã— å°±æ¥­çŠ¶æ³ã€‘")
    cross5d = desired_df.groupby(['target_muni', 'age_bucket', 'gender', 'has_national_license', 'employment_status']).size().reset_index(name='count')
    cross5d = cross5d.sort_values('count', ascending=False)

    print(f"  - ç·çµ„ã¿åˆã‚ã›æ•°: {len(cross5d):,}é€šã‚Š")
    print(f"  - ä¸Šä½10çµ„ã¿åˆã‚ã›:")
    print(cross5d.head(10).to_string(index=False))

    return desired_df


def explore_rarity_score(desired_df):
    """å¸Œå°‘æ€§ã‚¹ã‚³ã‚¢ã®ç®—å‡º"""

    print("\n\n" + "=" * 100)
    print("ã€å¸Œå°‘æ€§ã‚¹ã‚³ã‚¢ã®ç®—å‡ºã€‘")
    print("=" * 100)

    print("\nå¸Œå°‘ãªçµ„ã¿åˆã‚ã›ï¼ˆäººæç¢ºä¿ãŒå›°é›£ãªå¯èƒ½æ€§ï¼‰ã‚’ç‰¹å®š")

    # å¸‚ç”ºæ‘ Ã— å¹´é½¢å±¤ Ã— æ€§åˆ¥ Ã— å›½å®¶è³‡æ ¼ã®çµ„ã¿åˆã‚ã›ã§é›†è¨ˆ
    rarity = desired_df.groupby(['target_muni', 'age_bucket', 'gender', 'has_national_license']).size().reset_index(name='count')

    # å¸Œå°‘æ€§ã‚¹ã‚³ã‚¢ = 1 / countï¼ˆäººæ•°ãŒå°‘ãªã„ã»ã©é«˜ã‚¹ã‚³ã‚¢ï¼‰
    rarity['rarity_score'] = 1 / rarity['count']
    rarity = rarity.sort_values('rarity_score', ascending=False)

    print(f"\nã€æœ€ã‚‚å¸Œå°‘ãªçµ„ã¿åˆã‚ã›ï¼ˆä¸Šä½20ï¼‰ã€‘")
    print(rarity.head(20).to_string(index=False))

    # é€†ã«ã€æœ€ã‚‚ç«¶åˆãŒæ¿€ã—ã„çµ„ã¿åˆã‚ã›
    print(f"\nã€æœ€ã‚‚ç«¶åˆãŒæ¿€ã—ã„çµ„ã¿åˆã‚ã›ï¼ˆä¸‹ä½10ï¼‰ã€‘")
    print(rarity.tail(10).to_string(index=False))

    return rarity


def explore_supply_demand_gap(desired_df, analyzer):
    """éœ€çµ¦ã‚®ãƒ£ãƒƒãƒ—åˆ†æ"""

    print("\n\n" + "=" * 100)
    print("ã€éœ€çµ¦ã‚®ãƒ£ãƒƒãƒ—åˆ†æã€‘")
    print("=" * 100)

    df = analyzer.processed_data

    # å„å¸‚ç”ºæ‘ã¸ã®ã€Œéœ€è¦ã€ï¼ˆä½•äººãŒå¸Œæœ›ã—ã¦ã„ã‚‹ã‹ï¼‰
    demand = desired_df.groupby('target_muni').size().reset_index(name='demand_count')

    # å„å¸‚ç”ºæ‘ã‹ã‚‰ã®ã€Œä¾›çµ¦ã€ï¼ˆä½•äººãŒå±…ä½ã—ã¦ã„ã‚‹ã‹ï¼‰
    supply = df.groupby('residence_muni').size().reset_index(name='supply_count')
    supply.columns = ['target_muni', 'supply_count']

    # éœ€çµ¦ãƒãƒƒãƒãƒ³ã‚°
    gap = pd.merge(demand, supply, on='target_muni', how='outer').fillna(0)
    gap['demand_supply_ratio'] = gap['demand_count'] / (gap['supply_count'] + 1)  # ã‚¼ãƒ­é™¤ç®—å›é¿
    gap['gap'] = gap['demand_count'] - gap['supply_count']

    gap = gap.sort_values('demand_supply_ratio', ascending=False)

    print("\nã€éœ€è¦éå¤šã®å¸‚ç”ºæ‘ï¼ˆéœ€è¦ >> ä¾›çµ¦ï¼‰ã€‘")
    print("  â€» å¤–éƒ¨ã‹ã‚‰äººæã‚’é›†ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹åœ°åŸŸ")
    print(gap.head(15).to_string(index=False))

    print("\nã€ä¾›çµ¦éå¤šã®å¸‚ç”ºæ‘ï¼ˆä¾›çµ¦ >> éœ€è¦ï¼‰ã€‘")
    print("  â€» å±…ä½è€…ãŒå¤–éƒ¨ã«æµå‡ºã—ã¦ã„ã‚‹åœ°åŸŸ")
    gap_supply_over = gap.sort_values('demand_supply_ratio', ascending=True)
    print(gap_supply_over.head(15).to_string(index=False))

    return gap


def explore_qualification_locality(desired_df):
    """è³‡æ ¼åˆ¥ã®åœ°åŸŸé¸å¥½æ€§åˆ†æ"""

    print("\n\n" + "=" * 100)
    print("ã€è³‡æ ¼åˆ¥ã®åœ°åŸŸé¸å¥½æ€§åˆ†æã€‘")
    print("=" * 100)

    print("\nç‰¹å®šã®è³‡æ ¼ã‚’æŒã¤äººãŒã€ã©ã®éƒ½é“åºœçœŒã‚’å¥½ã‚€å‚¾å‘ãŒã‚ã‚‹ã‹")

    # desired_dfã«è³‡æ ¼æƒ…å ±ã‚’è¿½åŠ ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€å…ƒãƒ‡ãƒ¼ã‚¿ã¨çµåˆ
    # ï¼ˆç°¡ç•¥åŒ–ã®ãŸã‚ã€ã“ã“ã§ã¯è³‡æ ¼Ã—å¸‚ç”ºæ‘ã®åˆ†æã¯çœç•¥ã—ã€æ¦‚å¿µã‚’ç¤ºã™ï¼‰

    print("\nã€åˆ†æã‚¤ãƒ¡ãƒ¼ã‚¸ã€‘")
    print("  ä¾‹: ã€Œçœ‹è­·å¸«è³‡æ ¼ä¿æœ‰è€…ã®60%ã¯æ±äº¬ãƒ»ç¥å¥ˆå·ãƒ»åŸ¼ç‰ã‚’å¸Œæœ›ã€")
    print("  ä¾‹: ã€Œèª¿ç†å¸«è³‡æ ¼ä¿æœ‰è€…ã¯åœ°å…ƒå¿—å‘ãŒå¼·ãã€å±…ä½åœ°ã¨å¸Œæœ›åœ°ãŒä¸€è‡´ã™ã‚‹ç‡ãŒ80%ã€")
    print("  ä¾‹: ã€Œä¿è‚²å£«è³‡æ ¼ä¿æœ‰è€…ã¯20kmåœå†…ã®å‹¤å‹™åœ°ã‚’å¸Œæœ›ã™ã‚‹å‚¾å‘ï¼ˆå¹³å‡ç§»å‹•è·é›¢12kmï¼‰ã€")

    print("\n  â†’ ã“ã‚Œã‚’å…¨è³‡æ ¼Ã—å…¨å¸‚ç”ºæ‘ã§ç®—å‡ºã™ã‚Œã°ã€è³‡æ ¼åˆ¥ã®ç§»å‹•å‚¾å‘ãŒæ˜ç¢ºã«ãªã‚‹")


def explore_competition_analysis(desired_df):
    """ç«¶åˆåˆ†æï¼šåŒã˜å¸‚ç”ºæ‘ã‚’å¸Œæœ›ã™ã‚‹æ±‚è·è€…ã®ç‰¹æ€§"""

    print("\n\n" + "=" * 100)
    print("ã€ç«¶åˆåˆ†æã€‘")
    print("=" * 100)

    print("\nå„å¸‚ç”ºæ‘ã§ã€Œèª°ã¨ç«¶åˆã™ã‚‹ã‹ã€ã‚’åˆ†æ")

    # ä¸Šä½5å¸‚ç”ºæ‘ã‚’æŠ½å‡º
    top_munis = desired_df['target_muni'].value_counts().head(5).index

    for muni in top_munis:
        muni_data = desired_df[desired_df['target_muni'] == muni]

        print(f"\nã€{muni}ã€‘ã‚’å¸Œæœ›ã™ã‚‹æ±‚è·è€…ã®ç‰¹æ€§")
        print(f"  - ç·æ•°: {len(muni_data)}äºº")
        print(f"\n  å¹´é½¢å±¤åˆ†å¸ƒ:")
        age_dist = muni_data['age_bucket'].value_counts()
        for age, count in age_dist.items():
            print(f"    {age}: {count}äºº ({count/len(muni_data)*100:.1f}%)")

        print(f"\n  æ€§åˆ¥åˆ†å¸ƒ:")
        gender_dist = muni_data['gender'].value_counts()
        for gender, count in gender_dist.items():
            print(f"    {gender}: {count}äºº ({count/len(muni_data)*100:.1f}%)")

        print(f"\n  å›½å®¶è³‡æ ¼ä¿æœ‰ç‡: {muni_data['has_national_license'].mean():.1%}")

        print(f"\n  å°±æ¥­çŠ¶æ³:")
        emp_dist = muni_data['employment_status'].value_counts()
        for emp, count in emp_dist.items():
            print(f"    {emp}: {count}äºº ({count/len(muni_data)*100:.1f}%)")


def explore_network_centrality(desired_df, analyzer):
    """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æï¼šã©ã®å¸‚ç”ºæ‘ãŒäººæãƒ•ãƒ­ãƒ¼ã®ä¸­å¿ƒã‹"""

    print("\n\n" + "=" * 100)
    print("ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸­å¿ƒæ€§åˆ†æã€‘")
    print("=" * 100)

    df = analyzer.processed_data

    # å±…ä½åœ° â†’ å¸Œæœ›å‹¤å‹™åœ°ã®ãƒ•ãƒ­ãƒ¼ã‚’é›†è¨ˆ
    flows = []
    for idx, row in df.iterrows():
        residence = row['residence_muni']
        if pd.isna(residence):
            continue
        for area in row['desired_areas']:
            flows.append({
                'source': residence,
                'target': area['municipality'] if area['municipality'] else area['prefecture'],
                'count': 1
            })

    flows_df = pd.DataFrame(flows)
    flows_agg = flows_df.groupby(['source', 'target']).size().reset_index(name='flow_count')

    # å„å¸‚ç”ºæ‘ã®å…¥æ¬¡æ•°ï¼ˆä½•ç®‡æ‰€ã‹ã‚‰æµå…¥ãŒã‚ã‚‹ã‹ï¼‰
    in_degree = flows_agg.groupby('target')['source'].nunique().reset_index(name='in_degree')
    in_degree.columns = ['municipality', 'in_degree']
    in_degree = in_degree.sort_values('in_degree', ascending=False)

    print("\nã€å…¥æ¬¡æ•°ï¼ˆå¤šæ§˜ãªåœ°åŸŸã‹ã‚‰äººæãŒé›†ã¾ã‚‹å¸‚ç”ºæ‘ï¼‰ã€‘")
    print("  â€» äººæå¸å¼•åŠ›ãŒé«˜ã„åœ°åŸŸ")
    print(in_degree.head(20).to_string(index=False))

    # å„å¸‚ç”ºæ‘ã®å‡ºæ¬¡æ•°ï¼ˆä½•ç®‡æ‰€ã¸æµå‡ºã—ã¦ã„ã‚‹ã‹ï¼‰
    out_degree = flows_agg.groupby('source')['target'].nunique().reset_index(name='out_degree')
    out_degree.columns = ['municipality', 'out_degree']
    out_degree = out_degree.sort_values('out_degree', ascending=False)

    print("\nã€å‡ºæ¬¡æ•°ï¼ˆå¤šæ§˜ãªåœ°åŸŸã¸äººæãŒæµå‡ºã™ã‚‹å¸‚ç”ºæ‘ï¼‰ã€‘")
    print("  â€» äººæä¾›çµ¦æºã¨ã—ã¦é‡è¦ãªåœ°åŸŸ")
    print(out_degree.head(20).to_string(index=False))

    # åª’ä»‹ä¸­å¿ƒæ€§ã®ç°¡æ˜“ç‰ˆï¼ˆã©ã®å¸‚ç”ºæ‘ã‚’çµŒç”±ã™ã‚‹ã¨åŠ¹ç‡çš„ã‹ï¼‰
    print("\nã€ãƒãƒ–å¸‚ç”ºæ‘ï¼ˆå…¥æ¬¡æ•° Ã— å‡ºæ¬¡æ•°ãŒé«˜ã„ï¼‰ã€‘")
    hub = pd.merge(in_degree, out_degree, on='municipality', how='outer').fillna(0)
    hub['hub_score'] = hub['in_degree'] * hub['out_degree']
    hub = hub.sort_values('hub_score', ascending=False)
    print(hub.head(20).to_string(index=False))


def propose_new_features():
    """æ–°æ©Ÿèƒ½ææ¡ˆã‚µãƒãƒªãƒ¼"""

    print("\n\n" + "=" * 100)
    print("ã€æ–°æ©Ÿèƒ½ææ¡ˆã‚µãƒãƒªãƒ¼ã€‘")
    print("=" * 100)

    proposals = {
        '1. å¤šæ¬¡å…ƒã‚¯ãƒ­ã‚¹é›†è¨ˆã‚¨ãƒ³ã‚¸ãƒ³': {
            'ç¾çŠ¶': '2æ¬¡å…ƒï¼ˆå¸‚ç”ºæ‘Ã—ãƒšãƒ«ã‚½ãƒŠï¼‰',
            'ææ¡ˆ': '3-5æ¬¡å…ƒã®OLAPçš„é›†è¨ˆï¼ˆå¸‚ç”ºæ‘Ã—å¹´é½¢Ã—æ€§åˆ¥Ã—è³‡æ ¼Ã—å°±æ¥­çŠ¶æ³Ã—å±…ä½åœ°ï¼‰',
            'ä¾¡å€¤': 'ä»»æ„ã®åˆ‡ã‚Šå£ã§ãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã‚‰ã‚Œã‚‹æŸ”è»Ÿæ€§',
            'å·¥æ•°': '2æ—¥',
            'å‡ºåŠ›': 'MultiDimensionalCrossTab.csvï¼ˆãƒ”ãƒœãƒƒãƒˆå¯èƒ½ãªå½¢å¼ï¼‰'
        },
        '2. å¸Œå°‘æ€§ã‚¹ã‚³ã‚¢': {
            'ç¾çŠ¶': 'äººæ•°ã®ã¿è¡¨ç¤º',
            'ææ¡ˆ': 'å¸Œå°‘ãªçµ„ã¿åˆã‚ã›ã‚’å®šé‡åŒ–ï¼ˆæ¡ç”¨é›£æ˜“åº¦ã®æŒ‡æ¨™ï¼‰',
            'ä¾¡å€¤': 'ã€Œã“ã®æ¡ä»¶ã®äººæã¯å¸Œå°‘ã€ãŒä¸€ç›®ã§ã‚ã‹ã‚‹',
            'å·¥æ•°': '0.5æ—¥',
            'å‡ºåŠ›': 'RarityScore.csv'
        },
        '3. éœ€çµ¦ã‚®ãƒ£ãƒƒãƒ—åˆ†æ': {
            'ç¾çŠ¶': 'ãªã—',
            'ææ¡ˆ': 'å¸‚ç”ºæ‘ã”ã¨ã®éœ€è¦ï¼ˆå¸Œæœ›è€…æ•°ï¼‰ã¨ä¾›çµ¦ï¼ˆå±…ä½è€…æ•°ï¼‰ã®ã‚®ãƒ£ãƒƒãƒ—',
            'ä¾¡å€¤': 'ã€Œå¤–éƒ¨ã‹ã‚‰äººæã‚’é›†ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹åœ°åŸŸã€ãŒæ˜ç¢ºã«',
            'å·¥æ•°': '0.5æ—¥',
            'å‡ºåŠ›': 'SupplyDemandGap.csv'
        },
        '4. è³‡æ ¼åˆ¥åœ°åŸŸé¸å¥½æ€§': {
            'ç¾çŠ¶': 'Phase 7ã§è³‡æ ¼åˆ†å¸ƒã®ã¿',
            'ææ¡ˆ': 'å„è³‡æ ¼ä¿æœ‰è€…ãŒå¥½ã‚€éƒ½é“åºœçœŒãƒ»ç§»å‹•è·é›¢ã®å‚¾å‘',
            'ä¾¡å€¤': 'ã€Œçœ‹è­·å¸«ã¯50kmåœå†…ã€èª¿ç†å¸«ã¯20kmåœå†…ã€ãªã©å®Ÿå‹™çš„çŸ¥è¦‹',
            'å·¥æ•°': '1æ—¥',
            'å‡ºåŠ›': 'QualificationLocalityPreference.csv'
        },
        '5. ç«¶åˆåˆ†æ': {
            'ç¾çŠ¶': 'ãªã—',
            'ææ¡ˆ': 'åŒã˜å¸‚ç”ºæ‘ã‚’å¸Œæœ›ã™ã‚‹æ±‚è·è€…ã®è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«',
            'ä¾¡å€¤': 'ã€Œã“ã®å¸‚ç”ºæ‘ã§ã¯30ä»£å¥³æ€§ãŒå¤šãç«¶åˆã™ã‚‹ã€',
            'å·¥æ•°': '0.5æ—¥',
            'å‡ºåŠ›': 'CompetitionProfile.csv'
        },
        '6. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸­å¿ƒæ€§': {
            'ç¾çŠ¶': 'Phase 6ã§ãƒ•ãƒ­ãƒ¼é›†è¨ˆã®ã¿',
            'ææ¡ˆ': 'å…¥æ¬¡æ•°ãƒ»å‡ºæ¬¡æ•°ãƒ»ãƒãƒ–ã‚¹ã‚³ã‚¢ã§äººæãƒ•ãƒ­ãƒ¼ã®ä¸­å¿ƒåœ°ã‚’ç‰¹å®š',
            'ä¾¡å€¤': 'ã©ã®å¸‚ç”ºæ‘ãŒäººæãƒãƒ–ã‹ï¼ˆæ±äº¬ã€å¤§é˜ªãªã©ï¼‰',
            'å·¥æ•°': '1æ—¥',
            'å‡ºåŠ›': 'NetworkCentrality.csv'
        },
        '7. å‹•çš„ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³': {
            'ç¾çŠ¶': 'å›ºå®šã‚»ã‚°ãƒ¡ãƒ³ãƒˆï¼ˆå¹´é½¢å±¤Ã—æ€§åˆ¥Ã—è³‡æ ¼ï¼‰',
            'ææ¡ˆ': 'ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä»»æ„ã®æ¡ä»¶ã§ã‚»ã‚°ãƒ¡ãƒ³ãƒˆä½œæˆï¼ˆGAS UIï¼‰',
            'ä¾¡å€¤': 'ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªåˆ†æãŒå¯èƒ½',
            'å·¥æ•°': '2æ—¥ï¼ˆGASå´å®Ÿè£…å«ã‚€ï¼‰',
            'å‡ºåŠ›': 'DynamicSegmentation APIï¼ˆGASé–¢æ•°ï¼‰'
        },
        '8. æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ': {
            'ç¾çŠ¶': 'ãªã—ï¼ˆé™çš„ãƒ‡ãƒ¼ã‚¿ï¼‰',
            'ææ¡ˆ': 'è¤‡æ•°æ™‚ç‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¯”è¼ƒï¼ˆæ±‚è·è€…ã®å¸Œæœ›å¤‰åŒ–ï¼‰',
            'ä¾¡å€¤': 'ã€Œå¤ã«æ±äº¬å¸Œæœ›ãŒå¢—ãˆã‚‹ã€ãªã©å­£ç¯€æ€§ã®ç™ºè¦‹',
            'å·¥æ•°': '1.5æ—¥',
            'æ¡ä»¶': 'è¤‡æ•°æ™‚ç‚¹ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦',
            'å‡ºåŠ›': 'TrendAnalysis.csv'
        },
        '9. é¡ä¼¼ãƒšãƒ«ã‚½ãƒŠæ¤œç´¢': {
            'ç¾çŠ¶': 'ãªã—',
            'ææ¡ˆ': 'ã€Œã“ã®äººã¨ä¼¼ãŸç‰¹æ€§ã®äººã¯ã€ã©ã“ã§åƒããŸã„ã‹ã€ã‚’k-NNã§æ¤œç´¢',
            'ä¾¡å€¤': 'å€‹åˆ¥ã®æ±‚è·è€…ã«å¯¾ã™ã‚‹ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³',
            'å·¥æ•°': '1.5æ—¥',
            'å‡ºåŠ›': 'SimilarPersonaRecommendation APIï¼ˆGASé–¢æ•°ï¼‰'
        },
        '10. åŠ¹æœé‡ã¨å¤šé‡æ¯”è¼ƒè£œæ­£': {
            'ç¾çŠ¶': 'Phase 2ã§på€¤ã®ã¿',
            'ææ¡ˆ': 'Cohen\'s dã€CramÃ©r\'s Vã€Bonferroniè£œæ­£',
            'ä¾¡å€¤': 'çµ±è¨ˆçš„æœ‰æ„æ€§ã ã‘ã§ãªãã€å®Ÿå‹™çš„æ„ç¾©ã‚‚è©•ä¾¡',
            'å·¥æ•°': '0.5æ—¥',
            'å‡ºåŠ›': 'EnhancedStatisticalTests.csvï¼ˆPhase 2æ‹¡å¼µï¼‰'
        }
    }

    print("\nã€ææ¡ˆä¸€è¦§ã€‘")
    for i, (title, details) in enumerate(proposals.items(), 1):
        print(f"\n{title}")
        print("-" * 100)
        for key, value in details.items():
            print(f"  {key}: {value}")

    # å„ªå…ˆåº¦è©•ä¾¡
    print("\n\n" + "=" * 100)
    print("ã€å„ªå…ˆåº¦è©•ä¾¡ã€‘")
    print("=" * 100)

    print("\nğŸ”´ é«˜å„ªå…ˆåº¦ï¼ˆå³åŠ¹æ€§Ã—å®Ÿå‹™ä¾¡å€¤ãŒé«˜ã„ï¼‰")
    print("  1. éœ€çµ¦ã‚®ãƒ£ãƒƒãƒ—åˆ†æï¼ˆ0.5æ—¥ï¼‰")
    print("     â†’ ã€Œå¤–éƒ¨ã‹ã‚‰äººæã‚’é›†ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹åœ°åŸŸã€ãŒå³åº§ã«ã‚ã‹ã‚‹")
    print("\n  2. å¸Œå°‘æ€§ã‚¹ã‚³ã‚¢ï¼ˆ0.5æ—¥ï¼‰")
    print("     â†’ æ¡ç”¨é›£æ˜“åº¦ã®å®šé‡åŒ–ã€å®Ÿå‹™åˆ¤æ–­ã«ç›´çµ")
    print("\n  3. ç«¶åˆåˆ†æï¼ˆ0.5æ—¥ï¼‰")
    print("     â†’ å„å¸‚ç”ºæ‘ã§ã®ç«¶åˆçŠ¶æ³ã‚’æŠŠæ¡ã€æˆ¦ç•¥ç«‹æ¡ˆã«æœ‰ç”¨")

    print("\nğŸŸ¡ ä¸­å„ªå…ˆåº¦ï¼ˆãƒ‡ãƒ¼ã‚¿æ´»ç”¨ã®æ·±åŒ–ï¼‰")
    print("  4. å¤šæ¬¡å…ƒã‚¯ãƒ­ã‚¹é›†è¨ˆã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆ2æ—¥ï¼‰")
    print("     â†’ æŸ”è»Ÿãªãƒ‡ãƒ¼ã‚¿æ¢ç´¢ãŒå¯èƒ½ã«ãªã‚‹åŸºç›¤")
    print("\n  5. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸­å¿ƒæ€§ï¼ˆ1æ—¥ï¼‰")
    print("     â†’ äººæãƒ•ãƒ­ãƒ¼ã®æ§‹é€ ã‚’ç†è§£")
    print("\n  6. è³‡æ ¼åˆ¥åœ°åŸŸé¸å¥½æ€§ï¼ˆ1æ—¥ï¼‰")
    print("     â†’ è³‡æ ¼ã”ã¨ã®ç§»å‹•å‚¾å‘ã‚’æŠŠæ¡")

    print("\nğŸŸ¢ ä½å„ªå…ˆåº¦ï¼ˆæ¡ä»¶ä»˜ããƒ»é•·æœŸçš„ï¼‰")
    print("  7. å‹•çš„ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆ2æ—¥ï¼‰")
    print("     â†’ GAS UIé–‹ç™ºãŒå¿…è¦ã€ã‚„ã‚„å¤§æ›ã‹ã‚Š")
    print("\n  8. æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æï¼ˆ1.5æ—¥ï¼‰")
    print("     â†’ è¤‡æ•°æ™‚ç‚¹ã®ãƒ‡ãƒ¼ã‚¿ãŒå‰æ")
    print("\n  9. é¡ä¼¼ãƒšãƒ«ã‚½ãƒŠæ¤œç´¢ï¼ˆ1.5æ—¥ï¼‰")
    print("     â†’ æ©Ÿæ¢°å­¦ç¿’çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€ã‚„ã‚„é«˜åº¦")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""

    print("=" * 100)
    print("ãƒ‡ãƒ¼ã‚¿ã®æ½œåœ¨çš„ä¾¡å€¤æ¢ç´¢")
    print("=" * 100)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data_file = Path(r"C:\Users\fuji1\OneDrive\Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆä¿ç®¡\out\results_20251028_112441.csv")
    analyzer = PerfectJobSeekerAnalyzer(str(data_file))
    analyzer.load_data()
    analyzer.process_data()

    print(f"\nç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(analyzer.processed_data):,}ä»¶")

    # 1. ãƒ‡ãƒ¼ã‚¿æ¬¡å…ƒã®æ¢ç´¢
    desired_df = explore_data_dimensions(analyzer)

    # 2. å¸Œå°‘æ€§ã‚¹ã‚³ã‚¢
    rarity = explore_rarity_score(desired_df)

    # 3. éœ€çµ¦ã‚®ãƒ£ãƒƒãƒ—åˆ†æ
    gap = explore_supply_demand_gap(desired_df, analyzer)

    # 4. è³‡æ ¼åˆ¥åœ°åŸŸé¸å¥½æ€§ï¼ˆæ¦‚å¿µèª¬æ˜ï¼‰
    explore_qualification_locality(desired_df)

    # 5. ç«¶åˆåˆ†æ
    explore_competition_analysis(desired_df)

    # 6. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸­å¿ƒæ€§
    explore_network_centrality(desired_df, analyzer)

    # 7. æ–°æ©Ÿèƒ½ææ¡ˆ
    propose_new_features()


if __name__ == '__main__':
    main()
