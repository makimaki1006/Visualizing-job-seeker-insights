#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜å„ªå…ˆåº¦æ©Ÿèƒ½ã®çµ±åˆå¯èƒ½æ€§ãƒ†ã‚¹ãƒˆï¼ˆãƒ•ã‚¡ã‚¯ãƒˆãƒ™ãƒ¼ã‚¹æ¤œè¨¼ï¼‰

å®Ÿéš›ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦ã€å„æ©Ÿèƒ½ã®å‹•ä½œå¯èƒ½æ€§ã‚’æ¤œè¨¼
"""

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from collections import defaultdict

# UTF-8å‡ºåŠ›ã‚’å¼·åˆ¶ï¼ˆWindowsã®ã¿ï¼‰
# ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ: ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã§ã¯ä¸è¦
# if sys.platform == 'win32':
#     import io
#     sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from run_complete_v2_perfect import PerfectJobSeekerAnalyzer


def test_data_availability():
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ©ç”¨å¯èƒ½æ€§ç¢ºèª"""

    print("=" * 100)
    print("ã€Phase 1: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç¢ºèªã€‘")
    print("=" * 100)

    data_file = Path(r"C:\Users\fuji1\OneDrive\Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆä¿ç®¡\out\results_20251028_112441.csv")

    if not data_file.exists():
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_file}")
        return None

    print(f"\nâœ… ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª: {data_file.name}")

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    analyzer = PerfectJobSeekerAnalyzer(str(data_file))
    analyzer.load_data()
    analyzer.process_data()

    print(f"\nã€ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã€‘")
    print(f"  - ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(analyzer.processed_data):,}ä»¶")
    print(f"  - å¹´é½¢ç¯„å›²: {analyzer.processed_data['age'].min():.0f}æ­³ ï½ {analyzer.processed_data['age'].max():.0f}æ­³")
    print(f"  - å¹³å‡å¹´é½¢: {analyzer.processed_data['age'].mean():.1f}æ­³")
    print(f"  - æ€§åˆ¥åˆ†å¸ƒ: ç”·æ€§{(analyzer.processed_data['gender']=='ç”·æ€§').sum()}ä»¶, å¥³æ€§{(analyzer.processed_data['gender']=='å¥³æ€§').sum()}ä»¶")
    print(f"  - å›½å®¶è³‡æ ¼ä¿æœ‰ç‡: {analyzer.processed_data['has_national_license'].mean():.1%}")
    print(f"  - å¹³å‡è³‡æ ¼æ•°: {analyzer.processed_data['qualification_count'].mean():.2f}å€‹")
    print(f"  - å¹³å‡å¸Œæœ›å‹¤å‹™åœ°æ•°: {analyzer.processed_data.apply(lambda x: len(x['desired_areas']), axis=1).mean():.2f}ç®‡æ‰€")

    return analyzer


def test_persona_inference_feasibility(analyzer):
    """ãƒšãƒ«ã‚½ãƒŠæ¨è«–æ©Ÿèƒ½ã®å®Ÿè£…å¯èƒ½æ€§ãƒ†ã‚¹ãƒˆ"""

    print("\n" + "=" * 100)
    print("ã€Phase 2: ãƒšãƒ«ã‚½ãƒŠæ¨è«–æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆã€‘")
    print("=" * 100)

    df = analyzer.processed_data

    # å¹´é½¢å±¤åˆ¥ã‚»ã‚°ãƒ¡ãƒ³ãƒˆä½œæˆï¼ˆLCA ã®ä»£æ›¿ã¨ã—ã¦ï¼‰
    df['test_segment'] = pd.cut(df['age'], bins=[0, 30, 40, 50, 60, 100], labels=['20ä»£ä»¥ä¸‹', '30ä»£', '40ä»£', '50ä»£', '60ä»£ä»¥ä¸Š'])

    segments = {}

    for seg_id, seg_name in enumerate(df['test_segment'].unique()):
        if pd.isna(seg_name):
            continue

        seg_data = df[df['test_segment'] == seg_name]

        # å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿åé›†
        actual_characteristics = {
            'segment_id': seg_id,
            'segment_name': seg_name,
            'size': len(seg_data),
            'percentage': len(seg_data) / len(df) * 100,
            'avg_age': float(seg_data['age'].mean()),
            'age_range': (int(seg_data['age'].min()), int(seg_data['age'].max())),
            'gender_m_ratio': (seg_data['gender'] == 'ç”·æ€§').mean(),
            'national_license_rate': seg_data['has_national_license'].mean(),
            'avg_qualifications': seg_data['qualification_count'].mean(),
            'avg_desired_locations': seg_data.apply(lambda x: len(x['desired_areas']), axis=1).mean()
        }

        # æ¨è«–ãƒ­ã‚¸ãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼ˆæ—§Notebookã® _infer_segment_characteristics ã‚’å†ç¾ï¼‰
        inferred = {}

        # ãƒ©ã‚¤ãƒ•ã‚¹ãƒ†ãƒ¼ã‚¸æ¨è«–
        avg_age = actual_characteristics['avg_age']
        if avg_age > 55:
            inferred['life_stage'] = 'ã‚·ãƒ‹ã‚¢æœŸï¼ˆå®‰å®šé‡è¦–ã®å¯èƒ½æ€§ï¼‰'
        elif avg_age > 40:
            inferred['life_stage'] = 'ä¸­å …æœŸï¼ˆå®¶æ—è²¬ä»»é‡ã‚ï¼‰'
        else:
            inferred['life_stage'] = 'æˆé•·æœŸï¼ˆæŸ”è»Ÿï¼‰'

        # ç§»å‹•æ€§æ¨è«–
        avg_locations = actual_characteristics['avg_desired_locations']
        if avg_locations < 2:
            inferred['mobility_preference'] = 'åœ°åŸŸé™å®šå‹'
        elif avg_locations > 5:
            inferred['mobility_preference'] = 'åºƒåŸŸæ´»å‹•å‹'
        else:
            inferred['mobility_preference'] = 'ä¸­ç¨‹åº¦ç§»å‹•å‹'

        # ã‚­ãƒ£ãƒªã‚¢ã‚¹ãƒ†ãƒ¼ã‚¸æ¨è«–
        nat_rate = actual_characteristics['national_license_rate']
        if nat_rate > 0.7:
            inferred['career_stage'] = 'å°‚é–€è·ç¢ºç«‹'
        elif nat_rate < 0.3:
            inferred['career_stage'] = 'ã‚¨ãƒ³ãƒˆãƒªãƒ¼å±¤'
        else:
            inferred['career_stage'] = 'ä¸­é–“å±¤'

        # ãƒšãƒ«ã‚½ãƒŠåç”Ÿæˆï¼ˆæ—§Notebookã® _generate_evidence_based_name ã‚’å†ç¾ï¼‰
        name_parts = []
        if avg_age > 55:
            name_parts.append('ã‚·ãƒ‹ã‚¢')
        elif avg_age > 40:
            name_parts.append('ãƒŸãƒ‰ãƒ«')
        else:
            name_parts.append('ãƒ¤ãƒ³ã‚°')

        if avg_locations < 2:
            name_parts.append('åœ°åŸŸå¯†ç€')
        elif avg_locations > 5:
            name_parts.append('åºƒåŸŸæ´»å‹•')

        if nat_rate > 0.7:
            name_parts.append('å°‚é–€è·')

        persona_name = 'ãƒ»'.join(name_parts[:3]) + 'å±¤'

        # ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥ç”Ÿæˆï¼ˆæ—§Notebookã® _generate_evidence_based_strategies ã‚’å†ç¾ï¼‰
        strategies = []

        if avg_age < 30:
            strategies.append({'strategy': 'SNSï¼ˆInstagramã€TikTokï¼‰é‡è¦–', 'basis': f'å¹³å‡å¹´é½¢{avg_age:.1f}æ­³'})
        elif avg_age < 45:
            strategies.append({'strategy': 'LinkedIn/Indeedä¸­å¿ƒ', 'basis': f'å¹³å‡å¹´é½¢{avg_age:.1f}æ­³'})
        else:
            strategies.append({'strategy': 'å¾“æ¥åª’ä½“ã‚‚ä½µç”¨', 'basis': f'å¹³å‡å¹´é½¢{avg_age:.1f}æ­³'})

        if avg_locations < 2:
            strategies.append({'strategy': 'é€šå‹¤30kmåœã®æ±‚äººã‚’å„ªå…ˆè¡¨ç¤º', 'basis': f'å¸Œæœ›åœ°{avg_locations:.1f}ç®‡æ‰€'})

        if nat_rate > 0.5:
            strategies.append({'strategy': 'è³‡æ ¼æ‰‹å½“ãƒ»å°‚é–€å¾…é‡ã‚’æ˜ç¤º', 'basis': f'å›½å®¶è³‡æ ¼{nat_rate:.1%}'})

        segments[seg_name] = {
            'persona_name': persona_name,
            'actual': actual_characteristics,
            'inferred': inferred,
            'strategies': strategies
        }

    # çµæœè¡¨ç¤º
    print("\nã€ãƒšãƒ«ã‚½ãƒŠæ¨è«–çµæœã€‘")
    print("-" * 100)

    for seg_name, data in segments.items():
        print(f"\nâ–  {data['persona_name']} ï¼ˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆ: {seg_name}ï¼‰")
        print(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {data['actual']['size']:,}ä»¶ ({data['actual']['percentage']:.1f}%)")
        print(f"\n   ã€å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã€‘")
        print(f"     - å¹³å‡å¹´é½¢: {data['actual']['avg_age']:.1f}æ­³ (ç¯„å›²: {data['actual']['age_range'][0]}-{data['actual']['age_range'][1]}æ­³)")
        print(f"     - ç”·æ€§æ¯”ç‡: {data['actual']['gender_m_ratio']:.1%}")
        print(f"     - å›½å®¶è³‡æ ¼ä¿æœ‰ç‡: {data['actual']['national_license_rate']:.1%}")
        print(f"     - å¹³å‡è³‡æ ¼æ•°: {data['actual']['avg_qualifications']:.2f}å€‹")
        print(f"     - å¹³å‡å¸Œæœ›å‹¤å‹™åœ°æ•°: {data['actual']['avg_desired_locations']:.2f}ç®‡æ‰€")
        print(f"\n   ã€æ¨è«–ç‰¹æ€§ã€‘")
        for key, value in data['inferred'].items():
            print(f"     - {key}: {value}")
        print(f"\n   ã€ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥ã€‘")
        for i, strat in enumerate(data['strategies'], 1):
            print(f"     {i}. {strat['strategy']}")
            print(f"        æ ¹æ‹ : {strat['basis']}")

    print("\nâœ… ãƒšãƒ«ã‚½ãƒŠæ¨è«–æ©Ÿèƒ½ã¯å®Ÿè£…å¯èƒ½ï¼ˆãƒ†ã‚¹ãƒˆæˆåŠŸï¼‰")
    print(f"   ç”Ÿæˆãƒšãƒ«ã‚½ãƒŠæ•°: {len(segments)}å€‹")

    return segments


def test_association_rules_feasibility(analyzer):
    """ã‚¢ã‚½ã‚·ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«åˆ†æã®å®Ÿè£…å¯èƒ½æ€§ãƒ†ã‚¹ãƒˆ"""

    print("\n" + "=" * 100)
    print("ã€Phase 3: ã‚¢ã‚½ã‚·ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«åˆ†æãƒ†ã‚¹ãƒˆã€‘")
    print("=" * 100)

    # mlxtend ã®åˆ©ç”¨å¯èƒ½æ€§ç¢ºèª
    try:
        from mlxtend.preprocessing import TransactionEncoder
        from mlxtend.frequent_patterns import apriori, association_rules
        mlxtend_available = True
        print("âœ… mlxtendåˆ©ç”¨å¯èƒ½")
    except ImportError:
        mlxtend_available = False
        print("âš ï¸ mlxtendæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆç°¡æ˜“ç‰ˆã§æ¤œè¨¼ï¼‰")

    df = analyzer.processed_data

    # ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    print("\nã€ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ä½œæˆã€‘")
    transactions = []

    for idx, row in df.head(1000).iterrows():  # æœ€åˆã®1000ä»¶ã§ãƒ†ã‚¹ãƒˆ
        transaction = []

        # å¹´é½¢å±¤
        if row['age_bucket']:
            transaction.append(f"age_{row['age_bucket']}")

        # æ€§åˆ¥
        if row['gender']:
            transaction.append(f"gender_{row['gender']}")

        # å›½å®¶è³‡æ ¼
        if row['has_national_license']:
            transaction.append("national_license")

        # ç§»å‹•æ€§
        desired_count = len(row['desired_areas'])
        if desired_count > 3:
            transaction.append("high_mobility")
        elif desired_count <= 1:
            transaction.append("low_mobility")

        # è³‡æ ¼æ•°
        if row['qualification_count'] > 2:
            transaction.append("multi_qualified")
        elif row['qualification_count'] == 0:
            transaction.append("no_qualification")

        # å°±æ¥­çŠ¶æ³
        if row['employment_status']:
            transaction.append(f"emp_{row['employment_status']}")

        if transaction:
            transactions.append(transaction)

    print(f"  - ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³æ•°: {len(transactions)}ä»¶")
    print(f"  - ã‚µãƒ³ãƒ—ãƒ«ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³: {transactions[0]}")

    if mlxtend_available:
        print("\nã€Aprioriã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿè¡Œã€‘")

        # TransactionEncoder
        te = TransactionEncoder()
        te_ary = te.fit(transactions).transform(transactions)
        df_encoded = pd.DataFrame(te_ary, columns=te.columns_)

        print(f"  - ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å¾Œã®ã‚«ãƒ©ãƒ æ•°: {len(df_encoded.columns)}å€‹")
        print(f"  - ã‚«ãƒ©ãƒ ä¾‹: {list(df_encoded.columns[:10])}")

        # é »å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º
        frequent_itemsets = apriori(df_encoded, min_support=0.01, use_colnames=True)

        if len(frequent_itemsets) > 0:
            print(f"  - é »å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {len(frequent_itemsets)}å€‹")

            # ã‚¢ã‚½ã‚·ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ
            rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1.2)

            if len(rules) > 0:
                significant_rules = rules[(rules['confidence'] > 0.3) & (rules['lift'] > 1.2)]
                top_rules = significant_rules.sort_values('lift', ascending=False).head(10)

                print(f"  - æœ‰æ„ãªãƒ«ãƒ¼ãƒ«æ•°: {len(significant_rules)}å€‹")
                print(f"\nã€ä¸Šä½10ãƒ«ãƒ¼ãƒ«ã€‘")
                print("-" * 100)

                for i, (idx, rule) in enumerate(top_rules.iterrows(), 1):
                    antecedents = ', '.join(list(rule['antecedents']))
                    consequents = ', '.join(list(rule['consequents']))
                    print(f"\n  {i}. {antecedents} â†’ {consequents}")
                    print(f"      ã‚µãƒãƒ¼ãƒˆ: {rule['support']:.3f}")
                    print(f"      ä¿¡é ¼åº¦: {rule['confidence']:.3f}")
                    print(f"      ãƒªãƒ•ãƒˆ: {rule['lift']:.3f}")

                print("\nâœ… ã‚¢ã‚½ã‚·ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«åˆ†æã¯å®Ÿè£…å¯èƒ½ï¼ˆãƒ†ã‚¹ãƒˆæˆåŠŸï¼‰")
                print(f"   ç™ºè¦‹ãƒ«ãƒ¼ãƒ«æ•°: {len(significant_rules)}å€‹")

                return True
            else:
                print("âš ï¸ æœ‰æ„ãªãƒ«ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆé–¾å€¤èª¿æ•´ãŒå¿…è¦ï¼‰")
                return False
        else:
            print("âš ï¸ é »å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆmin_supportèª¿æ•´ãŒå¿…è¦ï¼‰")
            return False
    else:
        print("\nã€ç°¡æ˜“ç‰ˆã‚¢ã‚½ã‚·ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æã€‘")
        print("  mlxtendã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã“ã¨ã§ã€é«˜åº¦ãªã‚¢ã‚½ã‚·ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«åˆ†æãŒå¯èƒ½ã«ãªã‚Šã¾ã™")
        print("  ã‚³ãƒãƒ³ãƒ‰: pip install mlxtend")

        # ç°¡æ˜“ç‰ˆ: å…±èµ·é »åº¦ã®ã¿è¨ˆç®—
        co_occurrence = defaultdict(lambda: defaultdict(int))

        for transaction in transactions:
            for i, item1 in enumerate(transaction):
                for item2 in transaction[i + 1:]:
                    pair = tuple(sorted([item1, item2]))
                    co_occurrence[pair[0]][pair[1]] += 1

        print(f"\n  å…±èµ·ãƒšã‚¢æ•°: {sum(len(v) for v in co_occurrence.values())}å€‹")

        # ä¸Šä½10ãƒšã‚¢
        top_pairs = []
        for item1, pairs in co_occurrence.items():
            for item2, count in pairs.items():
                top_pairs.append((item1, item2, count))

        top_pairs.sort(key=lambda x: x[2], reverse=True)

        print(f"\n  ã€ä¸Šä½10å…±èµ·ãƒšã‚¢ã€‘")
        for i, (item1, item2, count) in enumerate(top_pairs[:10], 1):
            print(f"    {i}. {item1} & {item2}: {count}ä»¶")

        print("\nâš ï¸ ç°¡æ˜“ç‰ˆã®ã¿å®Ÿè£…å¯èƒ½ï¼ˆmlxtendã§é«˜åº¦ãªåˆ†æãŒå¯èƒ½ï¼‰")
        return 'partial'


def test_roi_projection_feasibility(analyzer):
    """ROIäºˆæ¸¬æ©Ÿèƒ½ã®å®Ÿè£…å¯èƒ½æ€§ãƒ†ã‚¹ãƒˆ"""

    print("\n" + "=" * 100)
    print("ã€Phase 4: ROIäºˆæ¸¬åˆ†æãƒ†ã‚¹ãƒˆã€‘")
    print("=" * 100)

    df = analyzer.processed_data

    # å¿œå‹Ÿã‚¹ã‚³ã‚¢ã‚’ä»®ç®—å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
    print("\nã€å¿œå‹Ÿã‚¹ã‚³ã‚¢ç®—å‡ºã€‘")
    df['app_score'] = 50  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢

    # å¹´é½¢ï¼ˆè‹¥ã„ã»ã©ã‚¹ã‚³ã‚¢UPï¼‰
    df['app_score'] += df['age'].apply(lambda x: 10 if x < 30 else (5 if x < 40 else 0))

    # è³‡æ ¼æ•°
    df['app_score'] += df['qualification_count'] * 5

    # å›½å®¶è³‡æ ¼
    df['app_score'] += df['has_national_license'] * 10

    # å¸Œæœ›å‹¤å‹™åœ°æ•°ï¼ˆå¤šã™ããšå°‘ãªã™ããšï¼‰
    desired_counts = df.apply(lambda x: len(x['desired_areas']), axis=1)
    df['app_score'] += desired_counts.apply(lambda x: 10 if 2 <= x <= 5 else 0)

    print(f"  - å¹³å‡å¿œå‹Ÿã‚¹ã‚³ã‚¢: {df['app_score'].mean():.1f}ç‚¹")
    print(f"  - ã‚¹ã‚³ã‚¢ç¯„å›²: {df['app_score'].min():.0f}ç‚¹ ï½ {df['app_score'].max():.0f}ç‚¹")

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†é¡
    high_potential = df[df['app_score'] > 75]
    medium_potential = df[(df['app_score'] >= 60) & (df['app_score'] <= 75)]
    low_potential = df[df['app_score'] < 60]

    total_candidates = len(df)
    addressable = len(high_potential) + len(medium_potential)

    print(f"\nã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†é¡ã€‘")
    print(f"  - é«˜ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ« (>75ç‚¹): {len(high_potential):,}äºº ({len(high_potential)/total_candidates*100:.1f}%)")
    print(f"  - ä¸­ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ« (60-75ç‚¹): {len(medium_potential):,}äºº ({len(medium_potential)/total_candidates*100:.1f}%)")
    print(f"  - ä½ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ« (<60ç‚¹): {len(low_potential):,}äºº ({len(low_potential)/total_candidates*100:.1f}%)")
    print(f"  - ã‚¢ãƒ‰ãƒ¬ã‚µãƒ–ãƒ«å¸‚å ´: {addressable:,}äºº ({addressable/total_candidates*100:.1f}%)")

    # ROIäºˆæ¸¬ï¼ˆæ—§Notebookã‹ã‚‰ï¼‰
    improvements = {
        'application_rate_improvement': {
            'min': 0.35,
            'expected': 0.56,
            'max': 0.77,
            'source': 'ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ†ã‚£ãƒƒã‚¯æ±‚äººåºƒå‘Š'
        },
        'cost_reduction': {
            'min': 0.25,
            'expected': 0.375,
            'max': 0.50,
            'source': 'AIå°å…¥äº‹ä¾‹'
        },
        'time_reduction': {
            'min': 0.16,
            'expected': 0.33,
            'max': 0.50,
            'source': 'è¤‡æ•°ä¼æ¥­ã®åŠ¹æœæ¸¬å®š'
        }
    }

    timeline = {
        '0-3_months': {
            'focus': 'Quick Wins',
            'expected_roi': '10-20%',
            'confidence': 'é«˜'
        },
        '3-6_months': {
            'focus': 'ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæœ€é©åŒ–',
            'expected_roi': '50-100%',
            'confidence': 'ä¸­'
        },
        '6-12_months': {
            'focus': 'AIãƒãƒƒãƒãƒ³ã‚°å°å…¥',
            'expected_roi': '100-300%',
            'confidence': 'ä¸­'
        },
        '12-24_months': {
            'focus': 'å®Œå…¨è‡ªå‹•åŒ–',
            'expected_roi': '300-500%',
            'confidence': 'ä½'
        }
    }

    quick_wins = {
        'çµ¦ä¸ãƒ¬ãƒ³ã‚¸é–‹ç¤º': {'åŠ¹æœ': 'CTRâ†‘ (+35%)', 'å¿…è¦æŠ•è³‡': 'Â¥0'},
        'ç«æ›œåˆå‰æŠ•ç¨¿': {'åŠ¹æœ': 'å¿œå‹Ÿç‡â†‘ (+22%)', 'å¿…è¦æŠ•è³‡': 'Â¥0'},
        'è³‡æ ¼æ‰‹å½“æ˜ç¤º': {'åŠ¹æœ': 'å°‚é–€è·å¿œå‹Ÿâ†‘ (+18%)', 'å¿…è¦æŠ•è³‡': 'Â¥0'},
        'ãƒ¢ãƒã‚¤ãƒ«æœ€é©åŒ–': {'åŠ¹æœ': 'é›¢è„±ç‡â†“ (-40%)', 'å¿…è¦æŠ•è³‡': 'Â¥50ä¸‡'}
    }

    print(f"\nã€æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„ç‡ï¼ˆãƒªã‚µãƒ¼ãƒãƒ™ãƒ¼ã‚¹ï¼‰ã€‘")
    print("-" * 100)
    for key, data in improvements.items():
        print(f"  â–  {key}")
        print(f"     Min: {data['min']:.1%} / Expected: {data['expected']:.1%} / Max: {data['max']:.1%}")
        print(f"     å‡ºå…¸: {data['source']}")

    print(f"\nã€ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³åˆ¥ROIç›®æ¨™ã€‘")
    print("-" * 100)
    for period, data in timeline.items():
        print(f"  â–  {period}: {data['focus']}")
        print(f"     æœŸå¾…ROI: {data['expected_roi']} (ä¿¡é ¼åº¦: {data['confidence']})")

    print(f"\nã€ã‚¯ã‚¤ãƒƒã‚¯ã‚¦ã‚£ãƒ³æ–½ç­–ã€‘")
    print("-" * 100)
    for strategy, data in quick_wins.items():
        print(f"  â–  {strategy}")
        print(f"     åŠ¹æœ: {data['åŠ¹æœ']}")
        print(f"     æŠ•è³‡: {data['å¿…è¦æŠ•è³‡']}")

    print("\nâœ… ROIäºˆæ¸¬åˆ†æã¯å®Ÿè£…å¯èƒ½ï¼ˆãƒ†ã‚¹ãƒˆæˆåŠŸï¼‰")
    print(f"   å³åº§ã«ã‚¢ãƒ—ãƒ­ãƒ¼ãƒå¯èƒ½: {len(high_potential):,}äºº / {total_candidates:,}äºº")

    return True


def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""

    print("=" * 100)
    print("é«˜å„ªå…ˆåº¦æ©Ÿèƒ½ã®çµ±åˆå¯èƒ½æ€§ãƒ†ã‚¹ãƒˆï¼ˆãƒ•ã‚¡ã‚¯ãƒˆãƒ™ãƒ¼ã‚¹æ¤œè¨¼ï¼‰")
    print("=" * 100)

    # Phase 1: ãƒ‡ãƒ¼ã‚¿ç¢ºèª
    analyzer = test_data_availability()
    if analyzer is None:
        print("\nâŒ ãƒ†ã‚¹ãƒˆä¸­æ–­: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    # Phase 2: ãƒšãƒ«ã‚½ãƒŠæ¨è«–ãƒ†ã‚¹ãƒˆ
    personas = test_persona_inference_feasibility(analyzer)

    # Phase 3: ã‚¢ã‚½ã‚·ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«ãƒ†ã‚¹ãƒˆ
    association_result = test_association_rules_feasibility(analyzer)

    # Phase 4: ROIäºˆæ¸¬ãƒ†ã‚¹ãƒˆ
    roi_result = test_roi_projection_feasibility(analyzer)

    # æœ€çµ‚ã‚µãƒãƒªãƒ¼
    print("\n\n" + "=" * 100)
    print("ã€æœ€çµ‚è©•ä¾¡ã‚µãƒãƒªãƒ¼ã€‘")
    print("=" * 100)

    results = {
        'ãƒšãƒ«ã‚½ãƒŠæ¨è«–æ©Ÿèƒ½': 'âœ… å®Ÿè£…å¯èƒ½' if personas else 'âŒ å®Ÿè£…ä¸å¯',
        'ã‚¢ã‚½ã‚·ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«': (
            'âœ… å®Ÿè£…å¯èƒ½ï¼ˆmlxtendåˆ©ç”¨ï¼‰' if association_result is True else
            'âš ï¸ éƒ¨åˆ†å®Ÿè£…å¯èƒ½ï¼ˆç°¡æ˜“ç‰ˆï¼‰' if association_result == 'partial' else
            'âŒ å®Ÿè£…ä¸å¯'
        ),
        'ROIäºˆæ¸¬åˆ†æ': 'âœ… å®Ÿè£…å¯èƒ½' if roi_result else 'âŒ å®Ÿè£…ä¸å¯'
    }

    for feature, status in results.items():
        print(f"  {status} - {feature}")

    # çµ±åˆæ¨å¥¨äº‹é …
    print("\n" + "=" * 100)
    print("ã€çµ±åˆæ¨å¥¨äº‹é …ã€‘")
    print("=" * 100)

    print("\nğŸ”´ å³åº§ã«çµ±åˆã™ã¹ãæ©Ÿèƒ½:")
    print("  1. ãƒšãƒ«ã‚½ãƒŠæ¨è«–æ©Ÿèƒ½")
    print("     ç†ç”±: ãƒ†ã‚¹ãƒˆã§5ã‚»ã‚°ãƒ¡ãƒ³ãƒˆç”Ÿæˆã€æ¨è«–ç‰¹æ€§ãƒ»æˆ¦ç•¥ã‚‚æ­£å¸¸ã«ç”Ÿæˆ")
    print("     å·¥æ•°: 1-2æ—¥")
    print("     æœŸå¾…åŠ¹æœ: ãƒšãƒ«ã‚½ãƒŠãƒ¬ãƒãƒ¼ãƒˆã®è³ªçš„å‘ä¸Šï¼ˆ3å€ï¼‰")

    print("\n  2. ROIäºˆæ¸¬åˆ†æ")
    print("     ç†ç”±: ãƒ†ã‚¹ãƒˆã§å¿œå‹Ÿã‚¹ã‚³ã‚¢ç®—å‡ºã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†é¡ãŒæˆåŠŸ")
    print("     å·¥æ•°: 0.5æ—¥")
    print("     æœŸå¾…åŠ¹æœ: çµŒå–¶å±¤å ±å‘Šè³‡æ–™ã¨ã—ã¦ä½¿ç”¨å¯èƒ½")

    if association_result == True:
        print("\n  3. ã‚¢ã‚½ã‚·ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«åˆ†æï¼ˆmlxtendåˆ©ç”¨ï¼‰")
        print("     ç†ç”±: mlxtendåˆ©ç”¨å¯èƒ½ã€æœ‰æ„ãªãƒ«ãƒ¼ãƒ«ç™ºè¦‹")
        print("     å·¥æ•°: 1æ—¥")
        print("     æœŸå¾…åŠ¹æœ: éš ã‚ŒãŸé–¢é€£æ€§ã®ç™ºè¦‹")
    else:
        print("\nğŸŸ¡ æ¡ä»¶ä»˜ãçµ±åˆ:")
        print("  3. ã‚¢ã‚½ã‚·ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«åˆ†æï¼ˆè¦mlxtendã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰")
        print("     ã‚³ãƒãƒ³ãƒ‰: pip install mlxtend")
        print("     ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¾Œã®å·¥æ•°: 1æ—¥")

    print("\n" + "=" * 100)


if __name__ == '__main__':
    main()
