#!/usr/bin/env python3
"""
è·ç¨®åˆ¥Tursoã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæœ€é©åŒ–ç‰ˆï¼‰

æœ€é©åŒ–å†…å®¹:
- è¤‡æ•°è¡ŒVALUESå¥ã«ã‚ˆã‚‹ä¸€æ‹¬INSERTï¼ˆ1æ–‡ã§100è¡ŒæŒ¿å…¥ï¼‰
- BATCH_SIZE: 2,000 â†’ 5,000
- MAX_WORKERS: 3 â†’ 6
- ç†è«–æ€§èƒ½: 300-600 rows/secï¼ˆå¾“æ¥æ¯” 5-10å€ï¼‰

ä½¿ç”¨æ–¹æ³•:
    python turso_job_type_import_optimized.py --job_type "çœ‹è­·å¸«" --csv_path "path/to/MapComplete.csv"
"""

import os
import sys
import json
import time
import argparse
import urllib.request
import urllib.error
from concurrent.futures import ThreadPoolExecutor, as_completed
from dotenv import load_dotenv
import pandas as pd

# æœ€é©åŒ–è¨­å®š
BATCH_SIZE = 10000          # 1ãƒãƒƒãƒã®ç·è¡Œæ•°ï¼ˆ5000â†’10000: 2å€ï¼‰
ROWS_PER_INSERT = 200       # 1ã¤ã®INSERTæ–‡ã§æŒ¿å…¥ã™ã‚‹è¡Œæ•°ï¼ˆ100â†’200: 2å€ï¼‰
MAX_WORKERS = 8             # ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆ6â†’8: é©åº¦ã«å¢—åŠ ï¼‰
MAX_RETRIES = 3             # ãƒªãƒˆãƒ©ã‚¤å›æ•°
REQUEST_TIMEOUT = 300       # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰

# è·ç¨®ã¨ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã®ãƒãƒƒãƒ”ãƒ³ã‚°
JOB_TYPE_PREFIX_MAP = {
    "ä»‹è­·è·": "",
    "çœ‹è­·å¸«": "NS",
    "ä¿è‚²å£«": "CH",
    "ç”Ÿæ´»ç›¸è«‡å“¡": "SW",
    "æ „é¤Šå£«": "DT",
    "ç®¡ç†æ „é¤Šå£«": "DT",
    "æ „é¤Šå£«ã€ç®¡ç†æ „é¤Šå£«": "DT",
    "ç†å­¦ç™‚æ³•å£«": "PT",
    "ä½œæ¥­ç™‚æ³•å£«": "OT",
    "ã‚±ã‚¢ãƒãƒã‚¸ãƒ£ãƒ¼": "CM",
    "ã‚µãƒ¼ãƒ“ã‚¹ç®¡ç†è²¬ä»»è€…": "SM",
    "ã‚µãƒ¼ãƒ“ã‚¹æä¾›è²¬ä»»è€…": "SP",
    "å­¦ç«¥æ”¯æ´": "AS",
    "èª¿ç†å¸«ã€èª¿ç†ã‚¹ã‚¿ãƒƒãƒ•": "CK",
}

# æ•°å€¤ã‚«ãƒ©ãƒ ã®ã‚»ãƒƒãƒˆï¼ˆé«˜é€Ÿãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—ç”¨ï¼‰
NUMERIC_COLS = frozenset([
    'desired_count', 'lat', 'lng', 'female_ratio', 'top_age_ratio',
    'avg_desired_areas', 'persona_count', 'persona_percentage',
    'supply_count', 'demand_count', 'gap', 'gap_ratio', 'rarity_score',
    'competition_score', 'flow_in', 'flow_out', 'net_flow',
    'latitude', 'longitude', 'avg_age', 'male_count', 'female_count',
    'applicant_count', 'avg_qualifications', 'count',
    'national_license_rate', 'total_in_municipality', 'market_share_pct',
    'avg_mobility_score', 'avg_urgency_score', 'inflow', 'outflow',
    'demand_supply_ratio', 'total_applicants',
    'male_ratio', 'top_employment_ratio', 'avg_qualification_count',
    'retention_rate', 'avg_reference_distance_km', 'median_distance_km',
    'mode_distance_km', 'min_distance_km', 'max_distance_km',
    'std_distance_km', 'q25_distance_km', 'q75_distance_km',
    'percentage', 'avg_desired_area_count'
])

INTEGER_COLS = frozenset(['id', 'desired_count'])


def get_turso_config():
    """Tursoæ¥ç¶šè¨­å®šã‚’å–å¾—"""
    load_dotenv()
    url = os.getenv('TURSO_DATABASE_URL', '').replace('libsql://', 'https://')
    token = os.getenv('TURSO_AUTH_TOKEN', '')
    return url, token


def execute_sql(url: str, token: str, sql: str, params: list = None) -> dict:
    """SQLã‚’å®Ÿè¡Œ"""
    stmt = {'sql': sql}
    if params:
        stmt['args'] = params

    req = urllib.request.Request(
        f'{url}/v2/pipeline',
        data=json.dumps({'requests': [{'type': 'execute', 'stmt': stmt}]}).encode(),
        headers={'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}
    )

    resp = urllib.request.urlopen(req, timeout=120)
    return json.loads(resp.read().decode())


def get_current_count(url: str, token: str, job_type: str = None) -> int:
    """ç¾åœ¨ã®è¡Œæ•°ã‚’å–å¾—"""
    if job_type:
        result = execute_sql(url, token,
            "SELECT COUNT(*) FROM job_seeker_data WHERE job_type = ?",
            [{'type': 'text', 'value': job_type}])
    else:
        result = execute_sql(url, token, 'SELECT COUNT(*) FROM job_seeker_data')
    return int(result['results'][0]['response']['result']['rows'][0][0]['value'])


def get_existing_ids_for_job_type(url: str, token: str, job_type: str) -> set:
    """æŒ‡å®šè·ç¨®ã®æ—¢å­˜IDã‚»ãƒƒãƒˆã‚’å–å¾—"""
    print(f"æ—¢å­˜IDç¢ºèªä¸­ (job_type: {job_type})...")

    result = execute_sql(url, token,
        "SELECT DISTINCT id FROM job_seeker_data WHERE job_type = ?",
        [{'type': 'text', 'value': job_type}])

    rows = result['results'][0]['response']['result'].get('rows', [])
    ids = set()
    for row in rows:
        if row and row[0]:
            ids.add(str(row[0].get('value', '')))
    print(f"æ—¢å­˜IDæ•°: {len(ids)}")
    return ids


def convert_value(val, col_name: str):
    """å€¤ã‚’Turso APIç”¨ã«å¤‰æ›ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
    # NULLåˆ¤å®š
    if pd.isna(val) or val is None or val == '' or str(val).lower() == 'nan':
        return {'type': 'null'}

    # æ•´æ•°ã‚«ãƒ©ãƒ 
    if col_name in INTEGER_COLS:
        try:
            return {'type': 'integer', 'value': str(int(float(val)))}
        except (ValueError, TypeError):
            return {'type': 'null'}

    # æ•°å€¤ã‚«ãƒ©ãƒ ï¼ˆREALå‹ï¼‰
    if col_name in NUMERIC_COLS:
        try:
            return {'type': 'float', 'value': float(val)}
        except (ValueError, TypeError):
            return {'type': 'null'}

    # ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ 
    return {'type': 'text', 'value': str(val)}


def build_multi_row_insert(columns: list, rows_data: list) -> tuple:
    """
    è¤‡æ•°è¡ŒVALUESå¥ã‚’æŒã¤INSERTæ–‡ã‚’æ§‹ç¯‰

    Returns:
        (sql, args): SQLæ–‡ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆ
    """
    col_names = ', '.join(columns)
    num_cols = len(columns)

    # å„è¡Œã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼: (?, ?, ?, ...)
    row_placeholder = '(' + ', '.join(['?' for _ in range(num_cols)]) + ')'
    # å…¨è¡Œã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼: (?, ?, ...), (?, ?, ...), ...
    all_placeholders = ', '.join([row_placeholder for _ in rows_data])

    sql = f"INSERT OR REPLACE INTO job_seeker_data ({col_names}) VALUES {all_placeholders}"

    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ•ãƒ©ãƒƒãƒˆåŒ–
    args = []
    for row in rows_data:
        for col in columns:
            args.append(convert_value(row.get(col), col))

    return sql, args


def insert_batch_optimized(url: str, token: str, batch_df: pd.DataFrame,
                           columns: list, batch_num: int) -> tuple:
    """
    æœ€é©åŒ–ãƒãƒƒãƒæŒ¿å…¥: è¤‡æ•°è¡ŒVALUESå¥ã‚’ä½¿ç”¨

    1ãƒãƒƒãƒ(5000è¡Œ)ã‚’ã€100è¡Œãšã¤ã®è¤‡æ•°INSERTæ–‡ã«åˆ†å‰²
    â†’ 50å€‹ã®INSERTæ–‡ã‚’1ã¤ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§é€ä¿¡
    """
    rows_data = batch_df.to_dict('records')
    total_rows = len(rows_data)

    # 100è¡Œãšã¤ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
    chunks = [rows_data[i:i+ROWS_PER_INSERT] for i in range(0, total_rows, ROWS_PER_INSERT)]

    # å„ãƒãƒ£ãƒ³ã‚¯ã®INSERTæ–‡ã‚’æ§‹ç¯‰
    requests = []
    for chunk in chunks:
        sql, args = build_multi_row_insert(columns, chunk)
        requests.append({
            'type': 'execute',
            'stmt': {'sql': sql, 'args': args}
        })

    # ãƒªãƒˆãƒ©ã‚¤ä»˜ãã§å®Ÿè¡Œ
    for attempt in range(MAX_RETRIES):
        try:
            req = urllib.request.Request(
                f'{url}/v2/pipeline',
                data=json.dumps({'requests': requests}).encode(),
                headers={'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}
            )
            resp = urllib.request.urlopen(req, timeout=REQUEST_TIMEOUT)
            result = json.loads(resp.read().decode())

            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
            errors = []
            for i, r in enumerate(result.get('results', [])):
                if r.get('type') == 'error':
                    errors.append(f"Chunk {i}: {r.get('error', {}).get('message', 'Unknown error')}")

            if errors:
                return (batch_num, total_rows, len(errors) * ROWS_PER_INSERT, errors[:3])

            return (batch_num, total_rows, 0, None)

        except urllib.error.HTTPError as e:
            error_body = e.read().decode() if hasattr(e, 'read') else str(e)
            if attempt < MAX_RETRIES - 1:
                time.sleep(2 ** attempt)
                continue
            return (batch_num, total_rows, total_rows, [f"HTTP {e.code}: {error_body[:200]}"])

        except Exception as e:
            if attempt < MAX_RETRIES - 1:
                time.sleep(2 ** attempt)
                continue
            return (batch_num, total_rows, total_rows, [str(e)[:200]])

    return (batch_num, 0, total_rows, ["Max retries exceeded"])


def main():
    parser = argparse.ArgumentParser(description='è·ç¨®åˆ¥Tursoã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆæœ€é©åŒ–ç‰ˆï¼‰')
    parser.add_argument('--job_type', type=str, required=True,
                        help='è·ç¨®å (ä¾‹: çœ‹è­·å¸«, ä¿è‚²å£«)')
    parser.add_argument('--csv_path', type=str, required=True,
                        help='ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    parser.add_argument('--dry_run', action='store_true',
                        help='ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆå®Ÿéš›ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ãªã„ï¼‰')
    parser.add_argument('--batch_size', type=int, default=BATCH_SIZE,
                        help=f'ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {BATCH_SIZE}ï¼‰')
    parser.add_argument('--workers', type=int, default=MAX_WORKERS,
                        help=f'ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {MAX_WORKERS}ï¼‰')
    args = parser.parse_args()

    job_type = args.job_type
    csv_path = args.csv_path
    batch_size = args.batch_size
    workers = args.workers

    print("=" * 60)
    print(f"è·ç¨®åˆ¥Tursoã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆæœ€é©åŒ–ç‰ˆï¼‰")
    print(f"  è·ç¨®: {job_type}")
    print(f"  CSVãƒ‘ã‚¹: {csv_path}")
    print(f"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size:,}")
    print(f"  ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼: {workers}")
    print(f"  1INSERTå½“ãŸã‚Šè¡Œæ•°: {ROWS_PER_INSERT}")
    print(f"  ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³: {args.dry_run}")
    print("=" * 60)

    # è¨­å®šå–å¾—
    url, token = get_turso_config()
    if not url or not token:
        print("ERROR: Tursoè¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        sys.exit(1)

    # CSVèª­ã¿è¾¼ã¿
    if not os.path.exists(csv_path):
        print(f"ERROR: CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")
        sys.exit(1)

    print(f"\nCSVèª­ã¿è¾¼ã¿ä¸­: {csv_path}")
    df = pd.read_csv(csv_path, dtype=str, low_memory=False)
    total_rows = len(df)
    print(f"CSVè¡Œæ•°ï¼ˆèª­ã¿è¾¼ã¿æ™‚ï¼‰: {total_rows:,}")

    # é‡è¤‡é™¤å»
    print("\né‡è¤‡é™¤å»å‡¦ç†ä¸­...")
    df_dedup = df.drop_duplicates()
    dup_count = total_rows - len(df_dedup)
    if dup_count > 0:
        print(f"[WARNING] é‡è¤‡è¡Œã‚’å‰Šé™¤: {dup_count:,} è¡Œ")
    df = df_dedup
    total_rows = len(df)
    print(f"CSVè¡Œæ•°ï¼ˆé‡è¤‡é™¤å»å¾Œï¼‰: {total_rows:,}")

    # job_typeåˆ—ã‚’è¿½åŠ 
    if 'job_type' not in df.columns:
        print(f"\njob_typeåˆ—ã‚’è¿½åŠ : '{job_type}'")
        df['job_type'] = job_type
    else:
        print(f"\njob_typeåˆ—ã¯æ—¢ã«å­˜åœ¨: '{df['job_type'].iloc[0]}'")

    # IDåˆ—ã®ç¢ºèª - ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ãIDã‚’ç”Ÿæˆ
    prefix = JOB_TYPE_PREFIX_MAP.get(job_type, "")
    if not prefix:
        print(f"[WARNING] è·ç¨® '{job_type}' ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        # è·ç¨®åã®å…ˆé ­2æ–‡å­—ã‚’ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã¨ã—ã¦ä½¿ç”¨
        prefix = job_type[:2].upper() if job_type else "XX"

    if 'id' not in df.columns:
        print(f"IDåˆ—ãŒãªã„ãŸã‚ç”Ÿæˆã—ã¾ã™ï¼ˆãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹: {prefix}ï¼‰")
        df['id'] = [f"{prefix}_{i}" for i in range(1, len(df) + 1)]
    else:
        # æ—¢å­˜IDã«ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ ï¼ˆã¾ã ä»˜ã„ã¦ã„ãªã„å ´åˆï¼‰
        df['id'] = df['id'].astype(str)
        if not df['id'].iloc[0].startswith(prefix):
            print(f"æ—¢å­˜IDã«ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹è¿½åŠ : {prefix}")
            df['id'] = df['id'].apply(lambda x: f"{prefix}_{x}")

    df['id'] = df['id'].astype(str)
    # ã‚µãƒ³ãƒ—ãƒ«IDç¢ºèª
    print(f"\nã‚µãƒ³ãƒ—ãƒ«ID (å…ˆé ­5ä»¶):")
    for i, id_val in enumerate(df['id'].head(5)):
        print(f"  {i+1}: {id_val}")

    # ç¾åœ¨ã®çŠ¶æ³ç¢ºèª
    current_count = get_current_count(url, token, job_type)
    print(f"\nTursoç¾åœ¨è¡Œæ•° (job_type='{job_type}'): {current_count:,}")

    total_count = get_current_count(url, token)
    print(f"Tursoç·è¡Œæ•°: {total_count:,}")

    # æ—¢å­˜IDã‚’å–å¾—ã—ã¦ã‚¹ã‚­ãƒƒãƒ—
    existing_ids = get_existing_ids_for_job_type(url, token, job_type)

    # æ–°è¦è¡Œã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
    new_df = df[~df['id'].isin(existing_ids)]
    new_rows = len(new_df)
    print(f"\næ–°è¦æŒ¿å…¥å¯¾è±¡: {new_rows:,} è¡Œ")

    if new_rows == 0:
        print("ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¸ˆã¿ã§ã™")
        return

    if args.dry_run:
        print("\n[ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³] å®Ÿéš›ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¯è¡Œã„ã¾ã›ã‚“")
        print(f"ã‚¤ãƒ³ãƒãƒ¼ãƒˆäºˆå®š: {new_rows:,} è¡Œ")
        estimated_time = new_rows / 400  # 400 rows/secæƒ³å®š
        print(f"æ¨å®šæ‰€è¦æ™‚é–“: {estimated_time/60:.1f}åˆ†ï¼ˆ400 rows/secæƒ³å®šï¼‰")
        return

    # ã‚«ãƒ©ãƒ ãƒªã‚¹ãƒˆ
    columns = list(df.columns)
    print(f"ã‚«ãƒ©ãƒ æ•°: {len(columns)}")

    # ãƒãƒƒãƒåˆ†å‰²
    batches = [new_df.iloc[i:i+batch_size] for i in range(0, new_rows, batch_size)]
    total_batches = len(batches)
    print(f"ãƒãƒƒãƒæ•°: {total_batches} (å„{batch_size:,}è¡Œ)")
    print(f"1ãƒãƒƒãƒã‚ãŸã‚ŠINSERTæ–‡æ•°: {batch_size // ROWS_PER_INSERT}")
    print("-" * 60)

    # ä¸¦åˆ—ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    start_time = time.time()
    completed = 0
    errors_total = 0
    rows_imported = 0

    with ThreadPoolExecutor(max_workers=workers) as executor:
        futures = {}
        for i, batch_df in enumerate(batches):
            future = executor.submit(
                insert_batch_optimized, url, token, batch_df, columns, i
            )
            futures[future] = i

        for future in as_completed(futures):
            batch_num, inserted, errors, error_msgs = future.result()
            completed += 1
            errors_total += errors
            rows_imported += inserted - errors

            elapsed = time.time() - start_time
            rows_done = min(completed * batch_size, new_rows)

            # é€²æ—è¨ˆç®—
            progress = (rows_done / new_rows) * 100
            rate = rows_imported / elapsed if elapsed > 0 else 0
            remaining = (new_rows - rows_done) / rate if rate > 0 else 0

            status = "OK" if errors == 0 else f"ERR({errors})"
            print(f"\r[{completed:4d}/{total_batches}] {progress:5.1f}% | "
                  f"{rate:,.0f} rows/sec | ETA {remaining/60:.1f}min | {status}", end="", flush=True)

            if error_msgs:
                print(f"\n  Error: {error_msgs[0][:100]}")

    # å®Œäº†
    elapsed = time.time() - start_time
    final_count = get_current_count(url, token, job_type)
    total_final = get_current_count(url, token)

    print("\n" + "=" * 60)
    print("ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰")
    print(f"  è·ç¨®: {job_type}")
    print(f"  å‡¦ç†æ™‚é–“: {elapsed/60:.1f}åˆ†")
    print(f"  å‡¦ç†è¡Œæ•°: {new_rows:,}")
    print(f"  å‡¦ç†é€Ÿåº¦: {new_rows/elapsed:,.0f} è¡Œ/ç§’")
    print(f"  ã‚¨ãƒ©ãƒ¼æ•°: {errors_total}")
    print(f"  è·ç¨®åˆ¥è¡Œæ•°: {final_count:,}")
    print(f"  ç·è¡Œæ•°: {total_final:,}")
    print("=" * 60)

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
    old_rate = 62  # å¾“æ¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿæ¸¬å€¤
    new_rate = new_rows / elapsed if elapsed > 0 else 0
    improvement = new_rate / old_rate if old_rate > 0 else 0
    print(f"\nğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ:")
    print(f"   å¾“æ¥: {old_rate} rows/sec")
    print(f"   ä»Šå›: {new_rate:,.0f} rows/sec")
    print(f"   æ”¹å–„: {improvement:.1f}å€")


if __name__ == "__main__":
    main()
